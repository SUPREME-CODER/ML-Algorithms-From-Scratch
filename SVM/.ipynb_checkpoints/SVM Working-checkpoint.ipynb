{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc161d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13872d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> FEATURE SELECTION << #\n",
    "# def remove_correlated_features(X):\n",
    "#     corr_threshold = 0.9\n",
    "#     corr = X.corr()\n",
    "#     drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
    "#     for i in range(corr.shape[0]):\n",
    "#         for j in range(i + 1, corr.shape[0]):\n",
    "#             if corr.iloc[i, j] >= corr_threshold:\n",
    "#                 drop_columns[j] = True\n",
    "#     columns_dropped = X.columns[drop_columns]\n",
    "#     X.drop(columns_dropped, axis=1, inplace=True)\n",
    "#     return columns_dropped\n",
    "\n",
    "del remove_correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "373f91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_less_significant_features(X, Y):\n",
    "#     sl = 0.05\n",
    "#     regression_ols = None\n",
    "#     columns_dropped = np.array([])\n",
    "#     for itr in range(0, len(X.columns)):\n",
    "#         regression_ols = sm.OLS(Y, X).fit()\n",
    "#         max_col = regression_ols.pvalues.idxmax()\n",
    "#         max_val = regression_ols.pvalues.max()\n",
    "#         if max_val > sl:\n",
    "#             X.drop(max_col, axis='columns', inplace=True)\n",
    "#             columns_dropped = np.append(columns_dropped, [max_col])\n",
    "#         else:\n",
    "#             break\n",
    "#     regression_ols.summary()\n",
    "#     return columns_dropped\n",
    "\n",
    "del remove_less_significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7881112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> MODEL TRAINING << #\n",
    "def compute_cost(W, X, Y):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
    "\n",
    "    # calculate cost\n",
    "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8c80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    # if only one example is passed (eg. in case of SGD)\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])  # gives multidimensional array\n",
    "\n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = np.zeros(len(W))\n",
    "\n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d) == 0:\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n",
    "        dw += di\n",
    "\n",
    "    dw = dw/len(Y_batch)  # average\n",
    "    return dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a3ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "#     nth = 0\n",
    "#     prev_cost = float(\"inf\")\n",
    "#     cost_threshold = 0.01  # in percent\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^nth epoch\n",
    "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "#             if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "#                 return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c91ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init():\n",
    "    print(\"reading dataset...\")\n",
    "    # read data in pandas (pd) data frame\n",
    "    data = pd.read_csv('data.csv')\n",
    "\n",
    "    # drop last column (extra column added by pd)\n",
    "    # and unnecessary first column (id)\n",
    "    data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
    "\n",
    "    print(\"applying feature engineering...\")\n",
    "    # convert categorical labels to numbers\n",
    "    diag_map = {'M': 1.0, 'B': -1.0}\n",
    "    data['diagnosis'] = data['diagnosis'].map(diag_map)\n",
    "\n",
    "    # put features & outputs in different data frames\n",
    "    Y = data.loc[:, 'diagnosis']\n",
    "    X = data.iloc[:, 1:]\n",
    "\n",
    "    # filter features\n",
    "#     remove_correlated_features(X)\n",
    "#     remove_less_significant_features(X, Y)\n",
    "\n",
    "    # normalize data for better convergence and to prevent overflow\n",
    "    X_normalized = MinMaxScaler().fit_transform(X.values)\n",
    "    X = pd.DataFrame(X_normalized)\n",
    "\n",
    "    # insert 1 in every row for intercept b\n",
    "    X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "\n",
    "    # split data into train and test set\n",
    "    print(\"splitting dataset into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n",
    "    print(X_train.head())\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    # train the model\n",
    "    print(\"training started...\")\n",
    "    W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(\"training finished.\")\n",
    "    print(\"weights are: {}\".format(W))\n",
    "\n",
    "    # testing the model\n",
    "    print(\"testing the model...\")\n",
    "    y_train_predicted = np.array([])\n",
    "    for i in range(X_train.shape[0]):\n",
    "        yp = np.sign(np.dot(X_train.to_numpy()[i], W))\n",
    "        y_train_predicted = np.append(y_train_predicted, yp)\n",
    "\n",
    "    y_test_predicted = np.array([])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
    "        y_test_predicted = np.append(y_test_predicted, yp)\n",
    "    \n",
    "\n",
    "    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "    print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
    "    print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025f33e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading dataset...\n",
      "applying feature engineering...\n",
      "splitting dataset into train and test sets...\n",
      "            0         1         2         3         4         5         6  \\\n",
      "68   0.096928  0.257694  0.103656  0.045387  0.487226  0.373965  0.733365   \n",
      "181  0.667755  0.570172  0.683505  0.495228  0.554934  0.809214  0.582709   \n",
      "63   0.103744  0.140345  0.106489  0.049799  0.221901  0.208975  0.140300   \n",
      "248  0.173648  0.524518  0.167369  0.086320  0.396678  0.162444  0.055740   \n",
      "60   0.150930  0.174839  0.143459  0.071432  0.548614  0.187811  0.025398   \n",
      "\n",
      "            7         8         9  ...        21        22        23  \\\n",
      "68   0.217445  0.530808  0.642376  ...  0.283316  0.075153  0.034285   \n",
      "181  0.743539  0.674242  0.505897  ...  0.571962  0.627970  0.467902   \n",
      "63   0.108350  0.646970  0.414280  ...  0.192164  0.075601  0.030697   \n",
      "248  0.080268  0.422727  0.280750  ...  0.617537  0.137308  0.066482   \n",
      "60   0.064115  0.850000  0.413648  ...  0.144723  0.096867  0.045075   \n",
      "\n",
      "           24        25        26        27        28        29  intercept  \n",
      "68   0.508684  0.397018  1.000000  0.601375  0.524936  0.409681          1  \n",
      "181  0.514627  0.709327  0.541534  0.997595  0.499310  0.481175          1  \n",
      "63   0.179555  0.136324  0.111581  0.174811  0.338459  0.195855          1  \n",
      "248  0.519910  0.109158  0.089856  0.210859  0.363493  0.173357          1  \n",
      "60   0.371987  0.069244  0.017316  0.088625  0.392667  0.165027          1  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "(455, 31)\n",
      "training started...\n",
      "Epoch is: 1 and Cost is: 5444.346934808045\n",
      "Epoch is: 2 and Cost is: 3468.1915914642177\n",
      "Epoch is: 4 and Cost is: 2429.2938538980343\n",
      "Epoch is: 8 and Cost is: 1919.1402513002897\n",
      "Epoch is: 16 and Cost is: 1520.889043158438\n",
      "Epoch is: 32 and Cost is: 1210.45759861679\n",
      "Epoch is: 64 and Cost is: 971.6265878540052\n",
      "Epoch is: 128 and Cost is: 807.5004156659264\n",
      "Epoch is: 256 and Cost is: 702.8692416175267\n",
      "Epoch is: 512 and Cost is: 647.5248556635368\n",
      "Epoch is: 1024 and Cost is: 625.5402056728213\n",
      "Epoch is: 2048 and Cost is: 610.0066334273049\n",
      "Epoch is: 4096 and Cost is: 606.2249911364914\n",
      "Epoch is: 4999 and Cost is: 604.3929838320907\n",
      "training finished.\n",
      "weights are: [ 1.32720136  0.84578431  1.1276105   2.15944432 -1.24360847 -3.23544997\n",
      "  3.29266461  6.83494498 -0.44162889  0.10408108  5.6848461  -1.91523567\n",
      "  3.27385409  3.77067568  1.66783542 -2.43918496 -1.76261637  0.84015293\n",
      " -1.95796181 -1.85414028  2.69807038  5.33585191  1.03844389  3.07567946\n",
      "  2.21801643 -0.62499499  2.67138058  0.01922802  4.66026452  2.16559984\n",
      " -9.24981129]\n",
      "testing the model...\n",
      "accuracy on test dataset: 0.9736842105263158\n",
      "recall on test dataset: 0.9534883720930233\n",
      "precision on test dataset: 0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "# set hyper-parameters and call init\n",
    "regularization_strength = 10000\n",
    "learning_rate = 0.000001\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
